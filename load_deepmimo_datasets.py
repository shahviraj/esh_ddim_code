"""
DeepMIMO Dataset Loader for Machine Learning
============================================

This module provides functions to load DeepMIMO datasets generated by the multiple
dataset generation scripts. It extracts channel matrices, user locations, and
antenna configuration parameters for ML training.

Author: Generated for DeepMIMO ML applications
Date: 2024
"""

import os
import re
import numpy as np
import scipy.io as sio
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
import warnings


def load_deepmimo_datasets(dataset_folder: str = "DeepMIMO_dataset", 
                          verbose: bool = True) -> Dict[str, any]:
    """
    Load all DeepMIMO datasets from the specified folder.
    
    Parameters
    ----------
    dataset_folder : str, optional
        Path to the folder containing .mat files, by default "DeepMIMO_dataset"
    verbose : bool, optional
        Whether to print progress information, by default True
    
    Returns
    -------
    Dict[str, any]
        Dictionary containing:
        - 'datasets': List of individual dataset dictionaries
        - 'configurations': DataFrame with configuration parameters
        - 'summary': Summary statistics
        - 'file_info': Information about loaded files
    """
    
    if verbose:
        print("=== DeepMIMO Dataset Loader ===")
        print(f"Loading datasets from: {dataset_folder}")
    
    # Check if folder exists
    if not os.path.exists(dataset_folder):
        raise FileNotFoundError(f"Dataset folder not found: {dataset_folder}")
    
    # Find all .mat files
    mat_files = [f for f in os.listdir(dataset_folder) 
                 if f.endswith('.mat') and f.startswith('DeepMIMO_dataset_config_')]
    
    if not mat_files:
        raise FileNotFoundError(f"No DeepMIMO dataset files found in {dataset_folder}")
    
    if verbose:
        print(f"Found {len(mat_files)} dataset files")
    
    # Load datasets
    datasets = []
    configurations = []
    file_info = []
    
    for i, mat_file in enumerate(sorted(mat_files)):
        if verbose:
            print(f"Loading {i+1}/{len(mat_files)}: {mat_file}")
        
        try:
            # Load the .mat file
            file_path = os.path.join(dataset_folder, mat_file)
            mat_data = sio.loadmat(file_path)
            
            # Debug: Print available keys
            if verbose:
                print(f"  Available keys in {mat_file}: {list(mat_data.keys())}")
            
            # Extract dataset and parameters
            if 'dataset' not in mat_data:
                raise KeyError(f"'dataset' key not found in {mat_file}")
            if 'dataset_params' not in mat_data:
                raise KeyError(f"'dataset_params' key not found in {mat_file}")
            
            dataset = mat_data['dataset'][0]  # Remove extra dimension
            params = mat_data['dataset_params'][0]  # Remove extra dimension
            
            # Debug: Check dataset structure
            if verbose:
                print(f"  Dataset type: {type(dataset)}")
                if hasattr(dataset, 'dtype') and dataset.dtype.names:
                    print(f"  Dataset fields: {dataset.dtype.names}")
                else:
                    print(f"  Dataset length: {len(dataset) if hasattr(dataset, '__len__') else 'N/A'}")
            
            # Parse configuration from filename
            config_info = parse_config_from_filename(mat_file)
            
            # Process the dataset
            processed_dataset = process_single_dataset(dataset, params, config_info)
            
            datasets.append(processed_dataset)
            configurations.append(config_info)
            file_info.append({
                'filename': mat_file,
                'file_path': file_path,
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024),
                'load_success': True
            })
            
        except Exception as e:
            if verbose:
                print(f"Warning: Failed to load {mat_file}: {str(e)}")
                import traceback
                traceback.print_exc()
            
            file_info.append({
                'filename': mat_file,
                'file_path': os.path.join(dataset_folder, mat_file),
                'file_size_mb': 0,
                'load_success': False,
                'error': str(e)
            })
    
    # Create configurations DataFrame
    config_df = pd.DataFrame(configurations)
    
    # Generate summary
    summary = generate_summary(datasets, config_df, file_info)
    
    if verbose:
        print_summary(summary)
    
    return {
        'datasets': datasets,
        'configurations': config_df,
        'summary': summary,
        'file_info': file_info
    }


def process_single_dataset(dataset: np.ndarray, 
                          params: np.ndarray, 
                          config_info: Dict) -> Dict[str, any]:
    """
    Process a single DeepMIMO dataset to extract ML-ready features.
    
    Parameters
    ----------
    dataset : np.ndarray
        Raw dataset from .mat file
    params : np.ndarray
        Parameters from .mat file
    config_info : Dict
        Configuration information parsed from filename
    
    Returns
    -------
    Dict[str, any]
        Processed dataset with ML-ready features
    """
    
    print(f"\n=== Processing Dataset: {config_info['filename']} ===")
    print(f"Configuration: BS[{config_info['bs_antennas_horizontal']},{config_info['bs_antennas_vertical']}] UE[{config_info['ue_antennas_horizontal']},{config_info['ue_antennas_vertical']}]")
    
    processed = {
        'config_info': config_info,
        'channel_matrices': [],
        'user_locations': [],
        'bs_locations': [],
        'los_status': [],
        'path_params': [],
        'user_rotations': [],
        'bs_rotations': [],
        'distances': [],
        'pathlosses': [],
        'num_users': 0,
        'num_bs': 0
    }
    
    # Extract base station information
    num_bs = len(dataset)
    processed['num_bs'] = num_bs
    
    for bs_idx in range(num_bs):
        bs_data = dataset[bs_idx]
        
        # Check if bs_data is valid
        if bs_data is None or not hasattr(bs_data, 'dtype') or bs_data.dtype.names is None:
            continue
        
        # BS location and rotation
        if 'loc' in bs_data.dtype.names and bs_data['loc'] is not None:
            processed['bs_locations'].append(bs_data['loc'][0])
        if 'rotation' in bs_data.dtype.names and bs_data['rotation'] is not None:
            processed['bs_rotations'].append(bs_data['rotation'][0])
        
        # Process users for this BS
        if 'user' in bs_data.dtype.names and bs_data['user'] is not None:
            users = bs_data['user'][0][0][0]
            if users is not None and hasattr(users, '__len__'):
                num_users = len(users)
                processed['num_users'] = max(processed['num_users'], num_users)
                
                print(f"  BS {bs_idx}: Found {num_users} users")
                
                # Debug first 3 users
                debug_users = min(3, num_users)
                print(f"  Debugging first {debug_users} users:")
                print(f"  Users array type: {type(users)}")
                print(f"  Users array length: {len(users) if hasattr(users, '__len__') else 'N/A'}")
                
                for user_idx in range(num_users):
                    user_data = users[user_idx]
                    
                    # Check if user_data is valid
                    if user_data is None:
                        if user_idx < debug_users:
                            print(f"    User {user_idx}: user_data is None")
                        continue
                    
                    if not hasattr(user_data, 'dtype'):
                        if user_idx < debug_users:
                            print(f"    User {user_idx}: user_data has no dtype attribute")
                            print(f"    User {user_idx}: user_data type: {type(user_data)}")
                        continue
                    
                    if user_data.dtype.names is None:
                        if user_idx < debug_users:
                            print(f"    User {user_idx}: user_data.dtype.names is None")
                            print(f"    User {user_idx}: user_data.dtype: {user_data.dtype}")
                            print(f"    User {user_idx}: user_data content: {user_data}")
                            print(f"    User {user_idx}: user_data shape: {user_data.shape if hasattr(user_data, 'shape') else 'N/A'}")
                        continue
                    
                    # Debug first 3 users
                    if user_idx < debug_users:
                        print(f"    User {user_idx}:")
                        print(f"      Available fields: {user_data.dtype.names}")
                    
                    # Channel matrix
                    if 'channel' in user_data.dtype.names and user_data['channel'] is not None:
                        channel = user_data['channel'][0][0]
                        # Reshape if needed (remove singleton dimensions)
                        while channel.ndim > 3 and channel.shape[-1] == 1:
                            channel = channel.squeeze(-1)
                        processed['channel_matrices'].append(channel)
                        
                        if user_idx < debug_users:
                            print(f"      Channel shape: {channel.shape}")
                            print(f"      Channel dtype: {channel.dtype}")
                            print(f"      Channel min/max magnitude: {np.min(np.abs(channel)):.6f}/{np.max(np.abs(channel)):.6f}")
                            print(f"      Channel sample values: {channel.flat[:5]}")
                    
                    # User location
                    if 'loc' in user_data.dtype.names and user_data['loc'] is not None:
                        loc = user_data['loc'][0][0]
                        processed['user_locations'].append(loc)
                        
                        if user_idx < debug_users:
                            print(f"      Location: {loc}")
                            print(f"      Location type: {type(loc)}")
                    
                    # LOS status
                    if 'LoS_status' in user_data.dtype.names and user_data['LoS_status'] is not None:
                        los = user_data['LoS_status'][0][0]
                        processed['los_status'].append(los)
                        
                        if user_idx < debug_users:
                            print(f"      LoS status: {los}")
                            print(f"      LoS type: {type(los)}")
                    
                    # Path parameters
                    if 'path_params' in user_data.dtype.names and user_data['path_params'] is not None:
                        path_params = user_data['path_params'][0][0]
                        processed['path_params'].append(path_params)
                        
                        if user_idx < debug_users:
                            print(f"      Path params type: {type(path_params)}")
                            if hasattr(path_params, 'dtype') and path_params.dtype.names:
                                print(f"      Path params fields: {path_params.dtype.names}")
                    
                    # User rotation
                    if 'rotation' in user_data.dtype.names and user_data['rotation'] is not None:
                        rotation = user_data['rotation'][0][0]
                        processed['user_rotations'].append(rotation)
                        
                        if user_idx < debug_users:
                            print(f"      Rotation: {rotation}")
                    
                    # Distance and pathloss
                    if 'distance' in user_data.dtype.names and user_data['distance'] is not None:
                        distance = user_data['distance'][0][0]
                        processed['distances'].append(distance)
                        
                        if user_idx < debug_users:
                            print(f"      Distance: {distance}")
                    
                    if 'pathloss' in user_data.dtype.names and user_data['pathloss'] is not None:
                        pathloss = user_data['pathloss'][0][0]
                        processed['pathlosses'].append(pathloss)
                        
                        if user_idx < debug_users:
                            print(f"      Pathloss: {pathloss}")
                    
                    if user_idx < debug_users:
                        print()  # Empty line for readability
    
    # Convert lists to numpy arrays where possible
    for key in ['channel_matrices', 'user_locations', 'bs_locations', 
                'los_status', 'user_rotations', 'bs_rotations', 
                'distances', 'pathlosses']:
        if processed[key]:
            try:
                processed[key] = np.array(processed[key])
            except:
                # Keep as list if conversion fails
                pass
    
    # Print summary of extracted data
    print(f"  Summary:")
    print(f"    Total users processed: {len(processed['channel_matrices'])}")
    print(f"    Channel matrices: {len(processed['channel_matrices'])}")
    print(f"    User locations: {len(processed['user_locations'])}")
    print(f"    LoS status: {len(processed['los_status'])}")
    print(f"    Path params: {len(processed['path_params'])}")
    print(f"    Distances: {len(processed['distances'])}")
    print(f"    Pathlosses: {len(processed['pathlosses'])}")
    
    
    print(f"    Channel matrix shape: {processed['channel_matrices'][0].shape}")
    print(f"    Channel matrix dtype: {processed['channel_matrices'][0].dtype}")
    
    
    print(f"    Location shape: {processed['user_locations'][0].shape}")
    print(f"    Location dtype: {processed['user_locations'][0].dtype}")
    
    print("=" * 60)
    
    return processed


def parse_config_from_filename(filename: str) -> Dict[str, any]:
    """
    Parse configuration parameters from filename.
    
    Parameters
    ----------
    filename : str
        Filename in format: DeepMIMO_dataset_config_{N}_BS{H}_{V}_UE{H}_{V}_spacing{BS}_{UE}.mat
    
    Returns
    -------
    Dict[str, any]
        Parsed configuration parameters
    """
    
    # Pattern to match: DeepMIMO_dataset_config_{N}_BS{H}_{V}_UE{H}_{V}_spacing{BS}_{UE}.mat
    pattern = r'DeepMIMO_dataset_config_(\d+)_BS(\d+)_(\d+)_UE(\d+)_(\d+)_spacing([\d.]+)_([\d.]+)\.mat'
    match = re.match(pattern, filename)
    
    if match:
        config_num, bs_h, bs_v, ue_h, ue_v, bs_spacing, ue_spacing = match.groups()
        return {
            'config_number': int(config_num),
            'bs_antennas_horizontal': int(bs_h),
            'bs_antennas_vertical': int(bs_v),
            'ue_antennas_horizontal': int(ue_h),
            'ue_antennas_vertical': int(ue_v),
            'bs_antenna_spacing': float(bs_spacing),
            'ue_antenna_spacing': float(ue_spacing),
            'filename': filename
        }
    else:
        # Fallback for different naming patterns
        return {
            'config_number': 0,
            'bs_antennas_horizontal': 0,
            'bs_antennas_vertical': 0,
            'ue_antennas_horizontal': 0,
            'ue_antennas_vertical': 0,
            'bs_antenna_spacing': 0.0,
            'ue_antenna_spacing': 0.0,
            'filename': filename
        }


def generate_summary(datasets: List[Dict], 
                    config_df: pd.DataFrame, 
                    file_info: List[Dict]) -> Dict[str, any]:
    """
    Generate summary statistics for the loaded datasets.
    
    Parameters
    ----------
    datasets : List[Dict]
        List of processed datasets
    config_df : pd.DataFrame
        Configuration DataFrame
    file_info : List[Dict]
        File information list
    
    Returns
    -------
    Dict[str, any]
        Summary statistics
    """
    
    successful_files = [f for f in file_info if f['load_success']]
    failed_files = [f for f in file_info if not f['load_success']]
    
    total_users = sum(d['num_users'] for d in datasets)
    total_bs = sum(d['num_bs'] for d in datasets)
    
    # Channel matrix statistics
    channel_shapes = []
    for dataset in datasets:
        
        for channel in dataset['channel_matrices']:
            channel_shapes.append(channel.shape)
    
    summary = {
        'total_files': len(file_info),
        'successful_files': len(successful_files),
        'failed_files': len(failed_files),
        'total_datasets': len(datasets),
        'total_users': total_users,
        'total_bs': total_bs,
        'unique_configurations': len(config_df),
        'channel_shapes': channel_shapes,
        'config_summary': config_df.describe() if not config_df.empty else None,
        'file_sizes_mb': [f['file_size_mb'] for f in successful_files],
        'failed_filenames': [f['filename'] for f in failed_files]
    }
    
    return summary


def print_summary(summary: Dict[str, any]) -> None:
    """
    Print a formatted summary of the loaded datasets.
    
    Parameters
    ----------
    summary : Dict[str, any]
        Summary statistics dictionary
    """
    
    print("\n=== Dataset Summary ===")
    print(f"Total files: {summary['total_files']}")
    print(f"Successfully loaded: {summary['successful_files']}")
    print(f"Failed to load: {summary['failed_files']}")
    print(f"Total datasets: {summary['total_datasets']}")
    print(f"Total users: {summary['total_users']}")
    print(f"Total base stations: {summary['total_bs']}")
    print(f"Unique configurations: {summary['unique_configurations']}")
    
    if summary['file_sizes_mb']:
        print(f"Total size: {sum(summary['file_sizes_mb']):.2f} MB")
        print(f"Average file size: {np.mean(summary['file_sizes_mb']):.2f} MB")
    
    if summary['channel_shapes']:
        unique_shapes = list(set(summary['channel_shapes']))
        print(f"Channel matrix shapes: {unique_shapes}")
    
    if summary['failed_filenames']:
        print(f"Failed files: {summary['failed_filenames']}")


def create_ml_dataset(loaded_data: Dict[str, any], 
                     target_feature: str = 'channel_matrices',
                     include_metadata: bool = True) -> Tuple[np.ndarray, np.ndarray, Dict]:
    """
    Create a machine learning ready dataset from loaded DeepMIMO data.
    
    Parameters
    ----------
    loaded_data : Dict[str, any]
        Data loaded by load_deepmimo_datasets()
    target_feature : str, optional
        Feature to use as target (X), by default 'channel_matrices'
    include_metadata : bool, optional
        Whether to include metadata features, by default True
    
    Returns
    -------
    Tuple[np.ndarray, np.ndarray, Dict]
        X (features), y (targets), metadata
    """
    
    datasets = loaded_data['datasets']
    configurations = loaded_data['configurations']
    
    X_list = []
    y_list = []
    metadata_list = []
    
    for i, dataset in enumerate(datasets):
        config = configurations.iloc[i]
        
        
        channels = dataset[target_feature]
        
        for j, channel in enumerate(channels):
            # Flatten channel matrix for ML
            X_list.append(channel.flatten())
            
            # Create metadata features
            if include_metadata:
                metadata = {
                    'config_id': i,
                    'user_id': j,
                    'bs_ant_h': config['bs_antennas_horizontal'],
                    'bs_ant_v': config['bs_antennas_vertical'],
                    'ue_ant_h': config['ue_antennas_horizontal'],
                    'ue_ant_v': config['ue_antennas_vertical'],
                    'bs_spacing': config['bs_antenna_spacing'],
                    'ue_spacing': config['ue_antenna_spacing']
                }
                
                # Add location if available
                if 'user_locations' in dataset and j < len(dataset['user_locations']):
                    metadata['user_location'] = dataset['user_locations'][j]
                
                # Add LOS status if available
                if 'los_status' in dataset and j < len(dataset['los_status']):
                    metadata['los_status'] = dataset['los_status'][j]
                
                metadata_list.append(metadata)
    
    X = np.array(X_list)
    y = X.copy()  # For autoencoder/unsupervised tasks, or modify as needed
    
    return X, y, metadata_list


# Example usage
if __name__ == "__main__":
    # Load all datasets
    data = load_deepmimo_datasets("DeepMIMO_dataset", verbose=True)
    
    # Create ML-ready dataset
    X, y, metadata = create_ml_dataset(data, include_metadata=True)
    
    print(f"\nML Dataset Shape: {X.shape}")
    print(f"Metadata entries: {len(metadata)}")
    
    # Example: Print first few metadata entries
    if metadata:
        print("\nFirst metadata entry:")
        for key, value in metadata[0].items():
            print(f"  {key}: {value}")
